---
title: Machine Learning Foundation 第二讲学习笔记
date: 2017-03-13 20:25:53
tags:
  - ml
---

## 内容回顾

从前一章节我们了解了机器学习的架构大致上就是 `A takes D and H to get g`, 也就是说我们会使用算法来基于资料与假设集合计算出一个符合资料呈现结果的函数式 g， 在这里我们就会看到 H 会长什么样子。然后介绍 Percepton Leaning Algorithm (**PLA**) 来让机器学习回答是非题，比如让机器回答银行是否要发信用卡给申请人这样的问题。

<!-- more -->

## 以信用卡为例

是否要发信用卡这个问题我们可以想成它是一个函数式 $f$， 而申请者的资料集合 $X$ 输入就可以得到 $Y$ ,$Y$ 是否核发信用卡的记录.

我们现在不知道 $f$， 将历史资料 $D$ 拿来当成训练资料，它会是一个多维向量，比如第一个维度是年龄，第二个维度是性别...等等。然后我们将这些资料 $D$ 及假设集合 $H$ 丢到机器学习算法 $A$，最后算出一个最像 $f$ 的 $g$，这个其实就是从假设集合 $H$ 挑出一个最好的假设的结果。

![](http://olkbjcb09.bkt.clouddn.com/blog/2017-03-13-131243.jpg)

## 简单的假设集合：感知器

要回答是否核发信用卡，可以用这样简单的想法来实现，利用多个维度进行加权求和，根据结果的大小来决定是否核发信用卡。

![](http://olkbjcb09.bkt.clouddn.com/blog/2017-06-12-160824.jpg)

对$h(x)$进行简化如下：

![](http://olkbjcb09.bkt.clouddn.com/blog/2017-06-12-164944.jpg)

一个具体的$h(x)$长什么样？

![](http://olkbjcb09.bkt.clouddn.com/blog/2017-06-12-165310.jpg)

- x 是什么（平面上的点）
- y 是什么（平面上点的状态）
- h 是什么（线）

## 过滤垃圾邮件

- 哪些字的权重更高？

![](http://olkbjcb09.bkt.clouddn.com/blog/2017-06-12-165659.jpg)


## 如何选择 $g$


理想状态下，$g = f$。但是 f 是未知的。我们的方法是：在已知的样本中，g 和 f 是相等的。以 g0 开始，在 D 中不断修正输出，总有更好的 g 存在，使得输出更接近 f。



![](http://olkbjcb09.bkt.clouddn.com/blog/2017-06-12-170603.jpg)

修正的理论依据

![](http://olkbjcb09.bkt.clouddn.com/blog/2017-06-12-173443.jpg)


修正过程如下：

![](http://olkbjcb09.bkt.clouddn.com/blog/2017-06-12-170650.jpg)

这里用到的算法成为 PLA，其主要优点是其非常直观的理解方式。但是经典的 PLA 存在两个问题：
- 算法可能永远都无法结束
- 即使知道算法最终能结束，当无法得知所需的时间

对于每一个样本都含有多个特征，且每个特征都有一个权重，表示该特征的重要程度。


##  PLA 一定会停吗？PLA 多久会停？

训练数据至少存在一条直线能将其切分开，称为训练数据是线性可分的 (Linear Seperable) 。线性可分的基础上我们任然无法确定具体什么时候算法能够停止。


## 线性不可分怎么办？

![](http://olkbjcb09.bkt.clouddn.com/blog/2017-06-12-173830.jpg)

针对这种噪声数据引起的，原本线性可分的训练数据变成了不是线性可分的情况，有一个升级版的 PLA 算法，只需要增加简单的两步节能解决问题：
- Step 1: 随便找一条线，即任意找一个 n 维向量 $w_0$，赋初值另 $w_0$。同时用一个变量 $w_{best}$ 表示在训练数据上表现最好的线，初始有 $w_{best} = w_0$。

- Step 2: 如果这条线正好把训练数据正确切分，训练结束！此时 $w$ 代表的 $h(x)$ 就是我们学得的模型 $g$。
- Step 3: 如果有任意一个样本没有被切分正确，即存在任意 $(x', y')$，使得 $sign(w^T x') \neq y'$，此时我们对 $w$ 代表的线做一点点修正，另 $w_{t+1} = w_t + y'x'$.


- Step 4: 如果修正后的 $w_{t+1}$ 比 $w_{best}$ 表现的更好，那么修改 $w_{best} = w_{t+1}$。

- Step 5: 如果训练轮数小于$N_{train}$，跳转到 Step 2。否则训练结束，此时 $w_{best}$ 代表的 $h(x)$ 就是我们学得的模型 $g$。


## 考虑噪声时的 PLA 和经典 PLA

![](http://olkbjcb09.bkt.clouddn.com/blog/2017-06-12-175152.jpg)
